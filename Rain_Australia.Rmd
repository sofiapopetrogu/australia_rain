---
title: "Rainfall_Predict"
author: "Sofia Trogu"
date: "2023-06-02"
output: pdf_document
---

```{r import}
library(corrplot)
library(ggplot2)
library(caret)
library(magrittr)
library(gridExtra)  
library(scales)
library(DMwR2)
library(UBL)
library(caret)
library(MASS)
library(ipred)
library(rsample)
library(mlr)
library(knitr)
library(glmnet)
```

## Download the Rain Dataset
```{r data loading}
file_path <- "/Users/LENOVO/Desktop/stat project/australia_rain/weatherAUS.csv"
rain <- read.csv(file_path)
head(rain)
summary(rain)
```

## Data Preprocessing

```{r data preprocessing}
# Find Empty Columns
empty_columns <- which(colSums(is.na(rain)) == nrow(rain))
names_of_empty_col<- names(rain)[empty_columns]
names_of_empty_col

dim(rain)

# Omit rows with NAs. We are left with 56,420 rows and 23 columns
rain <- na.omit(rain)
print(rain)

rain$RainToday <- ifelse(rain$RainToday == "Yes", 1,
                                     ifelse(rain$RainToday == "No", 0, rain$RainToday))

#RainTomorrow is our Target variable                         
rain$RainTomorrow <- ifelse(rain$RainTomorrow == "Yes", 1,
                                     ifelse(rain$RainTomorrow == "No", 0, rain$RainToday))  

#Remove date and location columns and hot encode wind directions
rain <- rain[, !(names(rain) %in% c('Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'))]

# 56,420 Ã— 18
head(rain)

rain <- as.data.frame(lapply(rain, as.numeric))
print(summary(rain))
```
## Correlation 
```{r correlation}
# Build a Correlation Matrix
cor_matrix <- cor(rain)

# Create a heatmap from the correlation matrix with blue, white, and green color scheme
heatmap(cor_matrix, col = colorRampPalette(c("blue", "white", "green"))(100))

# Add a color legend
legend_colors <- c("blue", "white", "green")
legend("right", legend = c(-1, 0, 1), fill = legend_colors, title = "Correlation")

# Add a main title
title(main = "Correlation Heatmap")

corrplot <- corrplot(cor(rain[,-19]),
             method = "number",
             diag = TRUE,
             tl.cex = 0.4,
             number.cex = 0.5,
             tl.col = "black")
```
## Density Plots

```{r density}
## Find features with highest correlation with target variable (RainTomorrow)

correlations <- cor_matrix['RainTomorrow',]
highly_correlated_columns <- correlations[abs(correlations) > 0.3 & correlations != 1] 
column_names <- names(highly_correlated_columns)
print(column_names)

rain_subset <- rain[,c(column_names)]

# We are trying to visualize relationship between Target Variable, RainTomorrow with the features having the highest correlation

# Calculate the count of each feature
count_rain_today <- sum(rain$RainToday == 1)
count_no_rain_today <- sum(rain$RainToday == 0)
count_rain_tomorrow <- sum(rain$RainTomorrow == 1)
count_no_rain_tomorrow <- sum(rain$RainTomorrow == 0)

# Create a data frame with the counts
count_df <- data.frame(
  Feature = c("RainToday", "RainTomorrow", "RainToday","RainTomorrow"),
  Value = c("1", "1", "0", "0"),
  Count = c(count_rain_today, count_rain_tomorrow, count_no_rain_today, count_no_rain_tomorrow)
)

plot_list <- list()

for (col in column_names) {
  if (col == "RainToday") {
    # Plot the barplot
    bar_plot <- ggplot(count_df, aes(x = Value, y = Count, fill = Feature)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(x = "Feature", y = "Count", fill = "") +
    scale_fill_manual(values = c("red", "blue"), labels = c("RainToday", "RainTomorrow")) +
    theme_minimal()
    plot_list <- append(plot_list, list(bar_plot))
  }
  else {
    density_plot <- rain%>% ggplot(aes(x = .data[[col]] , fill = factor(RainTomorrow))) +
    geom_density(alpha = 0.5) +
    labs(x = col, y = "Density", fill = "RainTomorrow") +
    ggtitle(paste("Density Plot of ", col, "by Raintomorrow")) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size =10))
  plot_list <- append(plot_list, list(density_plot))
  }
  
}
print(plot_list)
grid.arrange(grobs = plot_list, nrow = 3, ncol = 2)

# Sunshine: fraction of total days having higher sunshine record more 0 RainTomorrow, lower sunshine, more 1 RainTomorrow
# Humidity3pm: overlap more but still higher humidity associated with 1 RainTomorrow and vice versa
#Cloud9am/Cloud3pm: oscillates a bit across x-axis with higher discrepancies between RainTomorrow values at the two extremes.  
#RainToday: Since RainToday is a binary variable, the density plots are concentrated around 0 and 1. When RainToday =0, there is a bigger discrepancy between 0 and 1 for RainTomorrow. 

```
## Feature Scaling

```{r feature scaling}

# Check distribution of RainTomorrow values to see how balanced the data is
# 0: 43993; 1: 12427 
table(rain$RainTomorrow)

# Handling balancing of data below in train/test split

# Feature Scaling
min_max_norm <- function(x) {(x - min(x)) / (max(x) - min(x))}

rain_n <- as.data.frame(lapply(rain[,1:16], min_max_norm))

#Add back in Binary Feature, RainToday and target variable
rain_n$RainToday <- rain$RainToday
rain_n$RainTomorrow <- rain$RainTomorrow
```
##Logistic Regressiong without balancing or feature selection
```{r log. regression w/o balancing or feature selection}
#Test/Train Split
set.seed(123)

train0 <- sample(1:nrow(rain_n), nrow(rain_n) * 0.75)

# Calculate the test indices
test0 <- setdiff(1:nrow(rain_n), train0)

# Split the target variable into train and test sets
rain_n_train0 <- rain_n[train0,]
rain_n_test0 <- rain_n[test0 ,]
glm_model0 <- glm(data = rain_n_train0, 
        rain_n_train0$RainTomorrow ~ ., 
        family = binomial)

#R squared and Variance Inflation Factor (VIF)
#If the VIF value for a predictor variable is greater than 1, it indicates the presence of multicollinearity, suggesting that the predictor variable is highly correlated with other predictor variables in the model. 
model_summary0 <- summary(glm_model0)
summary(glm_model0)
r2_0 <- 1 - (model_summary0$deviance/model_summary0$null.deviance) # 0.3698141
vif0 <- 1/(1-r2_0) # 1.586833

#Predict test with model
glm_predict0 <- predict(glm_model0, rain_n_test0, type = "response")

#Convert predictions into 0,1 based on different thresholds

threshold4 <- 0.4
threshold5 <- 0.5
threshold6 <- 0.6

glm_predict_4_0<- ifelse(glm_predict0 > threshold4, 1, 0)
glm_predict_5_0<- ifelse(glm_predict0 > threshold5, 1, 0)
glm_predict_6_0<- ifelse(glm_predict0 > threshold6, 1, 0)

create_confusion_matrix <- function(confusion_matrix) {
  # Extract the confusion matrix table
  cm_table <- as.data.frame(confusion_matrix$table)
  
  #Extract F1 score
  f1_score <- confusion_matrix$byClass["F1"]
  
  # Plot the confusion matrix using ggplot2
  ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), color = "black", size = 8) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = paste("Confusion Matrix with F1-Score:", round(f1_score, 3)), x = "Target", y = "Prediction") +
    theme_minimal() +
    theme(axis.text = element_text(size = 8),
          plot.title = element_text(size = 8, face = "bold"))

}

# Confusion matrix with threshold = 0.4
table(rain_n_test0$RainTomorrow, glm_predict_4_0)
mean(glm_predict_4_0!=rain_n_test0$RainTomorrow)
cm4_0 <- confusionMatrix(data = factor(glm_predict_4_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
table(rain_n_test0$RainTomorrow, glm_predict_5_0)
mean(glm_predict_5_0!=rain_n_test0$RainTomorrow)
cm5_0 <- confusionMatrix(data = factor(glm_predict_5_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
table(rain_n_test0$RainTomorrow, glm_predict_6_0)
mean(glm_predict_6_0!=rain_n_test0$RainTomorrow)
cm6_0 <- confusionMatrix(data = factor(glm_predict_6_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')

a0 <- create_confusion_matrix(cm4_0)
b0 <- create_confusion_matrix(cm5_0)
c0 <- create_confusion_matrix(cm6_0)

# Threshold of 0.05 is the best among thresholds in terms of accuracy, sensitivity, and specificity
cm_all0 = list(a0, b0, c0)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = cm_all0, nrow = 3, width = plot_width)

```
## Balancing
```{r balancing}
#Downsamples majority class(0) 
#Added yname to specify the target variable in downSample function, ow it assumes first col is target
rain_balanced <- downSample(x = rain_n[, -which(names(rain_n) == "RainTomorrow")], 
                            y = factor(rain_n$RainTomorrow), 
                            yname = "RainTomorrow")
table(rain_balanced$RainTomorrow)
head(rain_balanced)

#Check if there are any NAs
sum(is.na(rain_balanced$RainTomorrow))

```
## Logistic Regression before Feature Selection
```{r log. regression pre feature selection}
#Test/Train Split
set.seed(123)

train_balanced <- sample(1:nrow(rain_balanced), nrow(rain_balanced) * 0.75)

# Calculate the test indices
test_balanced <- setdiff(1:nrow(rain_balanced), train_balanced)

# Split the target variable into train and test sets
rain_balanced_train <- rain_balanced[train_balanced,]
rain_balanced_test <- rain_balanced[test_balanced ,]
glm_model_balanced <- glm(data = rain_balanced_train, 
        rain_balanced_train$RainTomorrow ~ ., 
        family = binomial)

#R squared and Variance Inflation Factor (VIF)
#If the VIF value for a predictor variable is greater than 1, it indicates the presence of multicollinearity, suggesting that the predictor variable is highly correlated with other predictor variables in the model. 
model_summary_balanced <- summary(glm_model_balanced)
summary(glm_model_balanced)
r2_balanced <- 1 - (model_summary_balanced$deviance/model_summary_balanced$null.deviance) # 0.3849359
vif_balanced <- 1/(1-r2_balanced) # 1.625847

#Predict test with model
glm_predict_balanced <- predict(glm_model_balanced, rain_balanced_test, type = "response")

#Convert predictions into 0,1 based on different thresholds

glm_predict_4_balanced<- ifelse(glm_predict_balanced > threshold4, 1, 0)
glm_predict_5_balanced<- ifelse(glm_predict_balanced > threshold5, 1, 0)
glm_predict_6_balanced<- ifelse(glm_predict_balanced > threshold6, 1, 0)

# Confusion matrix with threshold = 0.4
table(rain_balanced_test$RainTomorrow, glm_predict_4_balanced)
mean(glm_predict_4_balanced!=rain_balanced_test$RainTomorrow)
cm4_balanced <- confusionMatrix(data = factor(glm_predict_4_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
table(rain_balanced_test$RainTomorrow, glm_predict_5_balanced)
mean(glm_predict_5_balanced!=rain_balanced_test$RainTomorrow)
cm5_balanced <- confusionMatrix(data = factor(glm_predict_5_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
table(rain_balanced_test$RainTomorrow, glm_predict_6_balanced)
mean(glm_predict_6_balanced!=rain_balanced_test$RainTomorrow)
cm6_balanced <- confusionMatrix(data = factor(glm_predict_6_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')

a_balanced <- create_confusion_matrix(cm4_balanced)
b_balanced <- create_confusion_matrix(cm5_balanced)
c_balanced <- create_confusion_matrix(cm6_balanced)

# Threshold of 0.05 is the best among thresholds in terms of accuracy, sensitivity, and specificity
cm_all_balanced = list(a_balanced, b_balanced, c_balanced)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = cm_all_balanced, nrow = 3, width = plot_width)

```
## Feature Selection (Backward Selection using BIC)
```{r feature selection}

# Perform logistic regression with backward stepwise selection
logit_model <- glm(rain_balanced$RainTomorrow ~ ., data = rain_balanced, family = binomial)  

# Perform forward stepwise selection using AIC with log(n)
logit_model <- stepAIC(logit_model, direction = "backward", k = log(nrow(rain_balanced)), trace = FALSE)

# Print the summary of the logistic regression model
summary(logit_model)

# Subset the dataframe with the chosen features based on stepwise selection
rain_subset <- rain_balanced[, c("MinTemp", "Sunshine", "WindGustSpeed", "WindSpeed9am", "WindSpeed3pm", "Humidity3pm", "Pressure9am", "Pressure3pm", "Cloud9am", "Cloud3pm", "Temp3pm", "RainToday", "RainTomorrow")]
                            
```
#Train/Test Split
```{r train/test split}

set.seed(123)

train <- sample(1:nrow(rain_subset), nrow(rain_subset) * 0.75)

# Calculate the test indices
test <- setdiff(1:nrow(rain_subset), train)

# Split the target variable into train and test sets
rain_subset_train <- rain_subset[train,]
rain_subset_test <- rain_subset[test,]

head(rain_subset_train)
head(rain_subset_test)
```
## Selected Model
```{r selected model}
# Model Definition
glm_model <- glm(data = rain_subset_train, 
        rain_subset_train$RainTomorrow ~ ., 
        family = binomial)

#R squared and Variance Inflation Factor (VIF)
#If the VIF value for a predictor variable is greater than 1, it indicates the presence of multicollinearity, suggesting that the predictor variable is highly correlated with other predictor variables in the model. 
model_summary <- summary(glm_model)
summary(glm_model)
r2 <- 1 - (model_summary$deviance/model_summary$null.deviance) # 0.3845522
vif <- 1/(1-r2) # 1.624833

#Predict test with model
glm_predict <- predict(glm_model, rain_subset_test, type = "response")

#Convert predictions into 0,1 based on different thresholds

glm_predict_4<- ifelse(glm_predict > threshold4, 1, 0)
glm_predict_5<- ifelse(glm_predict > threshold5, 1, 0)
glm_predict_6<- ifelse(glm_predict > threshold6, 1, 0)

# Confusion matrix with threshold = 0.4
table(rain_subset_test$RainTomorrow, glm_predict_4)
mean(glm_predict_4!=rain_subset_test$RainTomorrow)
cm4 <- confusionMatrix(data = factor(glm_predict_4), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
table(rain_subset_test$RainTomorrow, glm_predict_5)
mean(glm_predict_5!=rain_subset_test$RainTomorrow)
cm5 <- confusionMatrix(data = factor(glm_predict_5), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
table(rain_subset_test$RainTomorrow, glm_predict_6)
mean(glm_predict_6!=rain_subset_test$RainTomorrow)
cm6 <- confusionMatrix(data = factor(glm_predict_6), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

a <- create_confusion_matrix(cm4)
b <- create_confusion_matrix(cm5)
c <- create_confusion_matrix(cm6)

cm_all = list(a, b, c)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = cm_all, nrow = 3, width = plot_width)

```
##Comparison of 3 Models
```{r GLM Model Comparison}
## Summary Statistics: F1-Score

models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c("_balanced", "_0", "")

thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)

f1_scores <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), stringsAsFactors = FALSE)

for (model in models) {
  for (i in 1:length(thresholds)) {
    threshold <- thresholds[i]
    threshold_value <- threshold_values[i]
    
    # Calculate the F1 score for each combination of model and threshold
    cm_name <- paste0("cm", thresholds[i], model_suffix[i])
    cm <- get(cm_name)
    f1_score <- cm$byClass["F1"]
    
    # Add the F1 score to the data frame
    f1_scores <- rbind(f1_scores, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, stringsAsFactors = FALSE,row.names = NULL))
  }
}


f1_scores

```
##Predictions 
```{r prediction comparison for the 3 regression models}

# original classification :

ggplot(rain_n_test0 , aes(x = Rainfall,
y = Cloud9am,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))





```
## LASSO and Ridge Regression
```{r ridge regression and lasso}

set.seed(123)

X <- model.matrix(RainTomorrow~., data=rain_balanced)
X <- X[,-1]

X_train <- model.matrix(RainTomorrow~., data=rain_balanced_train)
X_test <- model.matrix(RainTomorrow~., data=rain_balanced_test)
X_train <- X_train[,-1]
X_test <- X_test[,-1]

# Target vector
y <- rain_balanced$RainTomorrow
y_train <- rain_balanced_train$RainTomorrow
y_test <- rain_balanced_test$RainTomorrow



# alpha=0 for ridge, alpha=1 (default) for lasso


# Ridge Regression (L2)

ridge_cv <- cv.glmnet(X_train, y_train, alpha=0, family = "binomial", type.measure = "class")
plot(ridge_cv)

# to select best lambda 
lambda_opt_ridge <- ridge_cv$lambda.min
lambda_opt_ridge

pred_ridge<- predict(ridge_cv, X_test, type = "class", s = lambda_opt_ridge)
table(y_test, pred_ridge)





#Lasso Regression (L1)
lasso_cv <- cv.glmnet(X_train, y_train, alpha=1, family = "binomial", type.measure = "class")
plot(lasso_cv)

lambda_opt_lasso <- lasso_cv$lambda.min
lambda_opt_lasso

pred_lasso<- predict(lasso_cv, X_test, type = "class", s = lambda_opt_lasso)
table(y_test, pred_lasso)


```
##TO DO: LAD, QDA, and KNN
```{r TODO}


```