---
title: "Will it Rain Tomorrow in Australia?"
authors: Sofia Pope Trogu, Fairouz Baz Radwan, Auriane Mahfouz
date: "2023-07-15"
output:
  pdf_document: 
    toc: true
    toc_depth: 2
    number_sections: true
    keep_tex: yes
    fig_caption: yes
latex_engine: pdflatex
classoption: landscape
---
## Import R Libraries
```{r import}
library(corrplot)
library(ggplot2)
library(caret)
library(magrittr)
library(gridExtra)  
library(scales)
library(DMwR2)
library(UBL)
library(caret)
library(MASS)
library(ipred)
library(rsample)
library(mlr)
library(knitr)
library(glmnet)
library(outliers)
library(class)
library(kableExtra)
```

## Download the Rain Dataset
We downloaded the Rain Australia dataset as a CSV from the following link: https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/code?datasetId=6012&searchQuery=visual. The dataset is originally composed of 23 columns and 145,460 examples. The aim of the data is to use available information about today's weather, i.e. Temperature, Humidity, Pressure, to predict whether it will rain tomorrow. Therefore, we originally start with 22 features and 1 target variable, RainTomorrow with binary classification (Yes=1, No=0). 
```{r data loading}
file_path <- "/Users/Sofia/Desktop/Rain_Australia/weatherAUS.csv"
rain <- read.csv(file_path)
head(rain)
summary(rain)
dim(rain)
```

##TODO: Create Feature Table of Contents
```{r load variable TOC}
file_path2 <- "/Users/Sofia/Desktop/Rain_Australia/Feature_TOC2.csv"
feature_toc <- read.csv(file_path2)

feature_toc %>%
  knitr::kable(
    format = "latex",
    align = "l",
    booktabs = TRUE,
    longtable = TRUE,
    linesep = "",
    caption = "Table of features with description and units."
    ) %>% row_spec(0,bold=TRUE) %>% 
  kableExtra::kable_styling(
      position = "left",
      latex_options = c("striped", "repeat_header"),
      stripe_color = "gray!15"
    )
```

## Data Pre-processing
Once we've loaded the dataset, we executed a series of pre-processing steps required prior to running our subsequent predictive models and data analysis. First, we found and removed any columns and rows that were completely empty. Next, we re-encoded the binary columns, RainToday and RainTomorrow to have numerical values: 1,0 instead of "Yes", "No", respectively. Then, we removed unneeded categorical variables, such as date, location, windgustdir, and winddir at 9 am and 3 pm. Finally, we reformated the dataframe to be all numeric values to be compatible with our further analysis methods. This process of data pre-processing led to a new dataset dimension of 56,420 × 18. 

```{r data preprocessing}
# Find Empty Columns
empty_columns <- which(colSums(is.na(rain)) == nrow(rain))
names_of_empty_col<- names(rain)[empty_columns]

dim(rain)

# Omit rows with NAs. We are left with 56,420 rows and 23 columns
rain <- na.omit(rain)

# Set Yes/No values to 1, 0, respectively for RainToday and RainTomorrow
rain$RainToday <- ifelse(rain$RainToday == "Yes", 1,
                                     ifelse(rain$RainToday == "No", 0, rain$RainToday))

#RainTomorrow is our Target variable                         
rain$RainTomorrow <- ifelse(rain$RainTomorrow == "Yes", 1,
                                     ifelse(rain$RainTomorrow == "No", 0, rain$RainToday))  

#Remove date, location columns, and wind direction columns
rain <- rain[, !(names(rain) %in% c('Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'))]

# New dimension of rain: 56,420 × 18
head(rain)

rain <- as.data.frame(lapply(rain, as.numeric))
summary(rain)
```
## Correlation 
After data pre-processing, we began to investigate the feature values and how each feature correlated with the other respective features. To perform this analysis, we built a correlation matrix that calculates the correlation values between each feature, so it results in a n_feature x n_feature matrix. From the correlation matrix, we visualized the correlations in both a heatmap and a numerical correlation plot.These plots provide a good context for the project and helped motivate our feature selection later on. 

```{r correlation}
# Build a Correlation Matrix
cor_matrix <- cor(rain)

# Create a heatmap from the correlation matrix with blue, white, and green color scheme
heatmap(cor_matrix, col = colorRampPalette(c("blue", "white", "green"))(100))

# Add a color legend to corr_matrix
legend_colors <- c("blue", "white", "green")
legend("right", legend = c(-1, 0, 1), fill = legend_colors, title = "Correlation")

# Add a main title
title(main = "Correlation Heatmap")

#Plot correlation matrix with numerical values
corrplot <- corrplot(cor(rain[,-19]),
             method = "number",
             diag = TRUE,
             tl.cex = 0.4,
             number.cex = 0.5,
             tl.col = "black")
```


## Density Plots
Based on the results of the correlation matrix, we further illustrated the features exhibiting the highest correlation using density and bar plots for the continuous and categorical variables, respectively against the target variable. From these plots, we identified the following trends: In the density plot with Sushine, we noted that the fraction of total days having higher sunshine are associated with more 0 RainTomorrow, and lower sunshine levels with more 1 RainTomorrow; Whereas, the Humidity3pm density plot appeared to have higher overlap between the two classes, but still higher humidity associated with 1 RainTomorrow and vice-versa. In the density plots with the two Cloud variables (9 am and 3 pm), we observed more oscillation across the x-axis with higher discrepancies in the target class at the two extremes. Finally, the representation of the RainToday binary variable against the target variable in a bar chart expresses the strong correlation between the classes of the two variables.We found a similar number of samples with both RainToday and RainTomorrow equal to 0 and vice-versa.   

```{r density}
## Find features with highest correlation with target variable (RainTomorrow)

correlations <- cor_matrix['RainTomorrow',]
highly_correlated_columns <- correlations[abs(correlations) > 0.3 & correlations != 1] 
column_names <- names(highly_correlated_columns)
print(column_names)

rain_subset <- rain[,c(column_names)]

# We are trying to visualize relationship between Target Variable, RainTomorrow with the features having the highest correlation

# Calculate the count of each feature
count_rain_today <- sum(rain$RainToday == 1)
count_no_rain_today <- sum(rain$RainToday == 0)
count_rain_tomorrow <- sum(rain$RainTomorrow == 1)
count_no_rain_tomorrow <- sum(rain$RainTomorrow == 0)

# Create a data frame with the counts
count_df <- data.frame(
  Feature = c("RainToday", "RainTomorrow", "RainToday","RainTomorrow"),
  Value = c("1", "1", "0", "0"),
  Count = c(count_rain_today, count_rain_tomorrow, count_no_rain_today, count_no_rain_tomorrow)
)

plot_list <- list()

for (col in column_names) {
  if (col == "RainToday") {
    # Plot the barplot
    bar_plot <- ggplot(count_df, aes(x = Value, y = Count, fill = Feature)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(x = "Feature", y = "Count", fill = "") +
    scale_fill_manual(values = c("red", "blue"), labels = c("RainToday", "RainTomorrow")) +
    theme_minimal()
    plot_list <- append(plot_list, list(bar_plot))
  }
  else {
    density_plot <- rain%>% ggplot(aes(x = .data[[col]] , fill = factor(RainTomorrow))) +
    geom_density(alpha = 0.5) +
    labs(x = col, y = "Density", fill = "RainTomorrow") +
    ggtitle(paste("Density Plot of ", col, "by Raintomorrow")) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size =10))
  plot_list <- append(plot_list, list(density_plot))
  }
  
}

# Visualize density and bar plots
grid.arrange(grobs = plot_list, nrow = 3, ncol = 2)



```
## Feature Scaling
One additional pre-processing step that is vital to statistical learning methodologies is the application of feature scaling. Particularly when we are creating similarity measures, i.e. Euclidean distances, correlations, it is important to ensure your feature values fall within a similar range. There are different feature scaling techniques to standardize the range of feature values, but we chose min/max normalization. The min/max scaling takes each feature value, subtracts the minimum feature value and divides the result by the difference between the maximum and minimum values. In our project, we applied the min/max scaling to all the continuous variables, but kept the two binary variables, RainToday and RainTomorrow (our target) the same. 
```{r feature scaling}


# Feature Scaling: Scale feature values using min/max scaling
min_max_norm <- function(x) {(x - min(x)) / (max(x) - min(x))}

rain_n <- as.data.frame(lapply(rain[,1:16], min_max_norm))

#Add back in Binary Features: RainToday and target variable, RainTomorrow
rain_n$RainToday <- rain$RainToday
rain_n$RainTomorrow <- rain$RainTomorrow
```
###Regression Models 
Our primary goal for this project was to build predictive models in order to implement a binary classification on our dataset. An intuitive approach to achieve that is the Logistic Regression model, which is a statistical modeling technique used to predict binary outcomes given a set of variables.
In our report, we chose to implement several versions of this model, where we incorporated the pre-processing techniques discussed in this report. We first started by applying a logistic regression on the original dataset, split into a train and test sets, with neither balancing nor feature selection. We then implemented two other models, the first with balancing alone and the second with both balancing and feature selection. Furthermore, to clearly see how effective each model is, we ran predictions on the test set, plotted the confusion matrices, and visualized different performance metrics such as f1-score, accuracy and error. For each of these predictions, we tried out different threshold values (0.4,0.5,0.6) for the final classification. 

##Train/Test split 
Prior to feeding our data to our model, we implemented the train/test split which is a common machine learning technique which involves splitting the original dataset into two subsets: the training set and the test set. The training set is usually made up of the majority of the dataset and is used for training, where the model learns the best hyperparameters, without playing any part in the evaluation phase. On the other hand, the test set is exclusively used for testing our model's performance at predicting classes.

##First Logistic Regression Model: without balancing or feature selection
```{r log. regression w/o balancing or feature selection}
#Train/Test Split


#Set Seed for Reproducibility 
set.seed(123)

# Set Training Set Size to 75% of Total Dataset
train0 <- sample(1:nrow(rain_n), nrow(rain_n) * 0.75)

# Calculate the test indices
test0 <- setdiff(1:nrow(rain_n), train0)

# Split the target variable into train and test sets
rain_n_train0 <- rain_n[train0,]
rain_n_test0 <- rain_n[test0 ,]

# Run GLM Logistic Regression Model using Training Set
glm_model0 <- glm(data = rain_n_train0, 
        rain_n_train0$RainTomorrow ~ ., 
        family = binomial)

# R squared and Variance Inflation Factor (VIF)

#R-squared (Coefficient of Determination): R-squared is a statistical metric that measures the proportion of the variance in the dependent variable (target variable) that can be explained by the independent variables (predictor variables) in a regression model. It provides an indication of how well the regression model fits the observed data.
# VIF: is a measure used to detect multicollinearity in a regression model. If the VIF value for a predictor variable is greater than 1, it indicates the presence of multicollinearity, suggesting that the predictor variable is highly correlated with other predictor variables in the model which makes it hard to assess the individual effects of each predictor on the dependent variable accurately. 


model_summary0 <- summary(glm_model0)
summary(glm_model0)

r2_0 <- 1 - (model_summary0$deviance/model_summary0$null.deviance) # 0.3698141
vif0 <- 1/(1-r2_0) # 1.586833

#Predict test using glm model
glm_predict0 <- predict(glm_model0, rain_n_test0, type = "response")

#Convert predictions into 0,1 based on different thresholds

threshold4 <- 0.4
threshold5 <- 0.5
threshold6 <- 0.6

glm_predict_4_0<- ifelse(glm_predict0 > threshold4, 1, 0)
glm_predict_5_0<- ifelse(glm_predict0 > threshold5, 1, 0)
glm_predict_6_0<- ifelse(glm_predict0 > threshold6, 1, 0)

# Function to create a confusion matrix with F1 score, error and accuracy rate
# A confusion matrix is a table that is often used to evaluate the performance of a classification model. It provides a summary of the predicted and actual values for a set of data points. The matrix allows us to assess how well the model is performing in terms of correctly and incorrectly classifying the data. The rows correspond to the predicted classes and the columns represent the actual targets.

create_confusion_matrix <- function(confusion_matrix, threshold, error, accuracy) {
  # Extract the confusion matrix table
  cm_table <- as.data.frame(confusion_matrix$table)
  
  #Extract F1 score
  f1_score <- confusion_matrix$byClass["F1"]
  
  # Plot the confusion matrix using ggplot2
  ggplot(data = cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), color = "black", size = 8) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = paste("Confusion Matrix for Threshold = ", threshold, "with F1-Score:", round(f1_score, 3), "Error:", round(error, 3) , "Accuracy:", round(accuracy, 3)), x = "Target", y = "Prediction") +
    theme_minimal() +
    theme(axis.text = element_text(size = 8),
          plot.title = element_text(size = 8, face = "bold"))

}

# Confusion matrix with threshold = 0.4
error4_0 <- mean(glm_predict_4_0!=rain_n_test0$RainTomorrow)
accuracy4_0 <- mean(glm_predict_4_0==rain_n_test0$RainTomorrow)
cm4_0 <- confusionMatrix(data = factor(glm_predict_4_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
error5_0 <- mean(glm_predict_5_0!=rain_n_test0$RainTomorrow)
accuracy5_0 <- mean(glm_predict_5_0==rain_n_test0$RainTomorrow)
cm5_0 <- confusionMatrix(data = factor(glm_predict_5_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
error6_0 <- mean(glm_predict_6_0!=rain_n_test0$RainTomorrow)
accuracy6_0 <- mean(glm_predict_6_0==rain_n_test0$RainTomorrow)
cm6_0 <- confusionMatrix(data = factor(glm_predict_6_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')

a0 <- create_confusion_matrix(cm4_0, 0.4, error4_0, accuracy4_0)
b0 <- create_confusion_matrix(cm5_0, 0.5, error5_0, accuracy5_0)
c0 <- create_confusion_matrix(cm6_0, 0.6, error6_0, accuracy6_0)

# Threshold of 0.05 is the best among thresholds in terms of accuracy, sensitivity, and specificity
cm_all0 = list(a0, b0, c0)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = cm_all0, nrow = 3, width = plot_width)

```
## Balancing
In addition to feature scaling, balancing our data is another technique used to prepare data before applying predictive modeling. Balancing reduces the effect of the dominating class which results in high bias and unreliable predictions. In order to balance data, there are several under-sampling and over-sampling methods that can be used depending on the characteristics of the dataset.
As we will observe in the code below, our original data isn't balanced, in which 0 is the majority class with over 40,000 and 1 has only about 12,000 samples. To account for this imbalance, we performed a method of under-sampling, whereby we reduced the number of samples in the majority class (0) to the same number of samples of the minority class (1). This methodology may reduce the pool of data we have available, but it removes the bias in our model predictions toward one dominating class. We opted against using over-sampling as that might add unwanted noise to our data which might also lead to unreliable results.

```{r balancing}


# Check distribution of RainTomorrow values to see how balanced the data is
# 0: 43993; 1: 12427 
table(rain$RainTomorrow)


#Downsamples majority class(0) 
#Added yname to specify the target variable in downSample function, ow it assumes first col is target
rain_balanced <- downSample(x = rain_n[, -which(names(rain_n) == "RainTomorrow")], 
                            y = factor(rain_n$RainTomorrow), 
                            yname = "RainTomorrow")
table(rain_balanced$RainTomorrow)
head(rain_balanced)

#Check if there are any NAs
sum(is.na(rain_balanced$RainTomorrow))

```

## Logistic Regression before Feature Selection
```{r log. regression pre feature selection}
#Test/Train Split
set.seed(123)

train_balanced <- sample(1:nrow(rain_balanced), nrow(rain_balanced) * 0.75)

# Calculate the test indices
test_balanced <- setdiff(1:nrow(rain_balanced), train_balanced)

# Split the target variable into train and test sets
rain_balanced_train <- rain_balanced[train_balanced,]
rain_balanced_test <- rain_balanced[test_balanced ,]
glm_model_balanced <- glm(data = rain_balanced_train, 
        rain_balanced_train$RainTomorrow ~ ., 
        family = binomial)

model_summary_balanced <- summary(glm_model_balanced)
summary(glm_model_balanced)
r2_balanced <- 1 - (model_summary_balanced$deviance/model_summary_balanced$null.deviance) # 0.3849359
vif_balanced <- 1/(1-r2_balanced) # 1.625847

#Predict test with model
glm_predict_balanced <- predict(glm_model_balanced, rain_balanced_test, type = "response")

#Convert predictions into 0,1 based on different thresholds

glm_predict_4_balanced<- ifelse(glm_predict_balanced > threshold4, 1, 0)
glm_predict_5_balanced<- ifelse(glm_predict_balanced > threshold5, 1, 0)
glm_predict_6_balanced<- ifelse(glm_predict_balanced > threshold6, 1, 0)

# Confusion matrix with threshold = 0.4
table(rain_balanced_test$RainTomorrow, glm_predict_4_balanced)
error4_balanced <- mean(glm_predict_4_balanced!=rain_balanced_test$RainTomorrow)
accuracy4_balanced <- mean(glm_predict_4_balanced==rain_balanced_test$RainTomorrow)
cm4_balanced <- confusionMatrix(data = factor(glm_predict_4_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
table(rain_balanced_test$RainTomorrow, glm_predict_5_balanced)
error5_balanced <- mean(glm_predict_5_balanced!=rain_balanced_test$RainTomorrow)
accuracy5_balanced <- mean(glm_predict_5_balanced==rain_balanced_test$RainTomorrow)
cm5_balanced <- confusionMatrix(data = factor(glm_predict_5_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
table(rain_balanced_test$RainTomorrow, glm_predict_6_balanced)
error6_balanced <- mean(glm_predict_6_balanced!=rain_balanced_test$RainTomorrow)
accuracy6_balanced <- mean(glm_predict_6_balanced==rain_balanced_test$RainTomorrow)
cm6_balanced <- confusionMatrix(data = factor(glm_predict_6_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')

a_balanced <- create_confusion_matrix(cm4_balanced, 0.4, error4_balanced, accuracy4_balanced)
b_balanced <- create_confusion_matrix(cm5_balanced, 0.5, error5_balanced, accuracy5_balanced)
c_balanced <- create_confusion_matrix(cm6_balanced, 0.6, error6_balanced, accuracy6_balanced)

# Threshold of 0.5 is the best among thresholds in terms of accuracy, sensitivity, and specificity
cm_all_balanced = list(a_balanced, b_balanced, c_balanced)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = cm_all_balanced, nrow = 3, width = plot_width)

```
##Feature Selection
Feature selection is another key step in predictive model definition. The selection method involves evaluating all the features provided in the original dataset and selecting the most relevant features that best describe and predict our data. By identifying a smaller subset of important features, we are ensuring that the model doesn't overfit on the training data and is able to generalize on new data. Several techniques can be implemented to apply feature selection. Forward and backward selection iterate through all the features to determine each feature's effect on the overall model by cumulatively adding features from an empty set or removing features one at a time from the full set, respectively. Moreover, the two methods can be integrated to obtain a more exhaustive feature selection. Within the configuration of forward and backward selection using Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), you can select the degree of penalization for the amount of features through the choice of k. The penalty term increases proportionally with the number of parameters set by k, which ensures that the complexity of the model is taken into account and to avoid overfitting. We chose to configure backward selection using BIC, which incorporates a log term and penalizes complex models more than AIC. 

## Feature Selection (Backward Selection using BIC)
```{r feature selection}

# Perform logistic regression with backward stepwise selection
logit_model <- glm(rain_balanced$RainTomorrow ~ ., data = rain_balanced, family = binomial)  

# Perform forward stepwise selection using BIC with log(n)
logit_model <- stepAIC(logit_model, direction = "backward", k = log(nrow(rain_balanced)), trace = FALSE)

# Print the summary of the logistic regression model
summary(logit_model)

# Subset the dataframe with the chosen features based on stepwise selection
rain_subset <- rain_balanced[, c("MinTemp", "Sunshine", "WindGustSpeed", "WindSpeed9am", "WindSpeed3pm", "Humidity3pm", "Pressure9am", "Pressure3pm", "Cloud9am", "Cloud3pm", "Temp3pm", "RainToday", "RainTomorrow")]
                            
```
#Train/Test Split
```{r train/test split}
set.seed(123)

train <- sample(1:nrow(rain_subset), nrow(rain_subset) * 0.75)

# Calculate the test indices
test <- setdiff(1:nrow(rain_subset), train)

# Split the target variable into train and test sets
rain_subset_train <- rain_subset[train,]
rain_subset_test <- rain_subset[test,]

head(rain_subset_train)
head(rain_subset_test)
```
## Selected Model
```{r selected model}
# Model Definition
glm_model <- glm(data = rain_subset_train, 
        rain_subset_train$RainTomorrow ~ ., 
        family = binomial)

model_summary <- summary(glm_model)
summary(glm_model)
r2 <- 1 - (model_summary$deviance/model_summary$null.deviance) # 0.3845522
vif <- 1/(1-r2) # 1.624833

#Predict test with model
glm_predict <- predict(glm_model, rain_subset_test, type = "response")

#Convert predictions into 0,1 based on different thresholds

glm_predict_4<- ifelse(glm_predict > threshold4, 1, 0)
glm_predict_5<- ifelse(glm_predict > threshold5, 1, 0)
glm_predict_6<- ifelse(glm_predict > threshold6, 1, 0)

# Confusion matrix with threshold = 0.4
table(rain_subset_test$RainTomorrow, glm_predict_4)
error4 <- mean(glm_predict_4!=rain_subset_test$RainTomorrow)
accuracy4 <- mean(glm_predict_4==rain_subset_test$RainTomorrow)
cm4 <- confusionMatrix(data = factor(glm_predict_4), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
table(rain_subset_test$RainTomorrow, glm_predict_5)
error5 <- mean(glm_predict_5!=rain_subset_test$RainTomorrow)
accuracy5 <- mean(glm_predict_5==rain_subset_test$RainTomorrow)
cm5 <- confusionMatrix(data = factor(glm_predict_5), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
table(rain_subset_test$RainTomorrow, glm_predict_6)
error6 <- mean(glm_predict_6!=rain_subset_test$RainTomorrow)
accuracy6 <- mean(glm_predict_6==rain_subset_test$RainTomorrow)
cm6 <- confusionMatrix(data = factor(glm_predict_6), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

a <- create_confusion_matrix(cm4, 0.4, error4, accuracy4)
b <- create_confusion_matrix(cm5, 0.5, error5, accuracy5)
c <- create_confusion_matrix(cm6, 0.6, error6, accuracy6)

cm_all = list(a, b, c)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = cm_all, nrow = 3, width = plot_width)

```
##Comparison of 3 Models
In the following plots, we compared each of the performance metrics previously computed across all the logistic regression models for each prediction threshold. By observing the results, we noticed close performance across all the models, with the simplest model before balancing and feature selection showing a slightly higher accuracy and F1 score, and lower error rate.

```{r GLM Model Comparison}
## Summary Statistics: F1-Score, Error, Accuracy

models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c( "_0", "_balanced", "")

thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)

metrics <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), Error = numeric(), Accuracy = numeric(), stringsAsFactors = FALSE)

j <-  1

for (model in models) {
  model_suff <- model_suffix[j]
  j <-  j + 1
  for (i in 1:length(thresholds)) {
    threshold <- thresholds[i]
    threshold_value <- threshold_values[i]

    # Calculate the F1 score for each combination of model and threshold
    cm_name <- paste0("cm", threshold, model_suff)
    cm <- get(cm_name)
    f1_score <- cm$byClass["F1"]
    
    error_name <- paste0("error", threshold, model_suff)
    error <- get(error_name)

    accuracy_name <- paste0("accuracy", threshold, model_suff)
    accuracy <- get(accuracy_name)
    
    # Add the F1 score to the data frame
    metrics <- rbind(metrics, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, Error = error, Accuracy = accuracy, stringsAsFactors = FALSE,row.names = NULL))
    
  }
  
}

metrics
```
##Visualizing data classifications from predictions
In addition to comparing performance metrics across the three models, we also sought to visualize the separation of classes in scatter plots between two predictive features: Sunshine and Temp3pm. As seen for the majority of the features, we are unable to completely separate the classes in the scatterplots since the features are highly correlated. 

```{r Visualizing data classifications}

#for different features: mintemp an temp3pm
a00 <- ggplot(data = rain_n_test0 , aes(x = MinTemp,
y = Temp3pm,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Mintemp",
y = "Temp3pm",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))

# original classification 
a0 <- ggplot(data = rain_n_test0 , aes(x = Sunshine,
y = Temp3pm,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Sunshine",
y = "Temp3pm",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))

# Simple GLM: Threshold of 0.5
a2 <- ggplot(data = rain_n_test0 , aes(x = Sunshine,
y = Temp3pm,
color= as.factor(glm_predict_5_0) )) +
geom_point()+
labs(x = "Sunshine",
y = "Temp3pm",
color = "Rain Tomorrow",
title = "Simple GLM: 0.5") +
theme(legend.position = c(0.8, 0.8))

# GLM with Balancing: Threshold of 0.5
b2 <- ggplot(data = rain_balanced_test , aes(x = Sunshine,
y = Temp3pm,
color= as.factor(glm_predict_5_balanced) )) +
geom_point()+
labs(x = "Sunshine",
y = "Temp3pm",
color = "Rain Tomorrow",
title = "GLM with Balancing: 0.5") +
theme(legend.position = c(0.8, 0.8))

# GLM with Balancing and Feature Selection
c2 <- ggplot(data = rain_balanced_test , aes(x = Sunshine,
y = Temp3pm,
color= as.factor(glm_predict_5) )) +
geom_point()+
labs(x = "Sunshine",
y = "Temp3pm",
color = "Rain Tomorrow",
title = "GLM with Balancing and Feature Selection: 0.5") +
theme(legend.position = c(0.8, 0.8))

grid.arrange(a0, a2, b2, c2, nrow = 2)

```

```{r logistic regression plots}
predicted_data<- data.frame(prob_of_rain = glm_predict0, RainTomorrow = rain_n_test0$RainTomorrow)
predicted_data<- predicted_data[order(predicted_data$prob_of_rain, decreasing = FALSE),]
predicted_data$rank<-  1:nrow(predicted_data)

a3<- ggplot(data = predicted_data, aes(x = rank, y = prob_of_rain)) +
  geom_point(aes(color = as.factor(RainTomorrow)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index") +
  ylab("Predicted probability") +
  ggtitle("Estimated Logistic Curve - Simple GLM") +
  labs(color = "Rain Tomorrow") 

predicted_data<- data.frame(prob_of_rain = glm_predict_balanced, RainTomorrow = rain_balanced_test$RainTomorrow)
predicted_data<- predicted_data[order(predicted_data$prob_of_rain, decreasing = FALSE),]
predicted_data$rank<-  1:nrow(predicted_data)

b3<- ggplot(data = predicted_data, aes(x = rank, y = prob_of_rain)) +
  geom_point(aes(color = as.factor(RainTomorrow)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index") +
  ylab("Predicted probability") +
  ggtitle("Estimated Logistic Curve - Simple GLM with Balancing") +
  labs(color = "Rain Tomorrow") 

predicted_data<- data.frame(prob_of_rain = glm_predict, RainTomorrow = rain_balanced_test$RainTomorrow)
predicted_data<- predicted_data[order(predicted_data$prob_of_rain, decreasing = FALSE),]
predicted_data$rank<-  1:nrow(predicted_data)

c3<- ggplot(data = predicted_data, aes(x = rank, y = prob_of_rain)) +
  geom_point(aes(color = as.factor(RainTomorrow)), alpha = 1, shape = 1, stroke = 1) +
  xlab("Index") +
  ylab("Predicted probability") +
  ggtitle("Estimated Logistic Curve - Simple GLM with Balancing and Feature Selection") +
  labs(color = "Rain Tomorrow") 

grid.arrange(a3, b3, c3, nrow = 3)
```

```{r X and y for Subset}
X_train_subset <- model.matrix(RainTomorrow~., data=rain_subset_train)
X_test_subset <- model.matrix(RainTomorrow~., data=rain_subset_test)
X_train_subset <- X_train_subset[,-1]
X_test_subset <- X_test_subset[,-1]

# Target vector
y_train_subset <- rain_subset_train$RainTomorrow
y_test_subset <- rain_subset_test$RainTomorrow
```

## LASSO and Ridge Regression
Beyond the standard logistic regression models, we chose to implement two alternative regularization models: LASSO and Ridge Regression. Both regularization techniques introduce a penalty team to the standard regression model; Least Absolute Shrinkage and Selection Operator (LASSO) regression utilizes the L1 norm, which is the sum of the absolute value of the parameters multiplied by a hyperparamter, lambda, whereas Ridge Regression uses the L2 norm, which is the square root of the sum of squared parameters multiplied by the lambda hyperparameter. The lambda hyperparameter plays a major role in determining the level of regularization introduced in the model; the higher the lambda value is, the higher the regularization and vice-versa. 
Moreover, LASSO regression is suitable for performing another form of feature selection as it introduces sparsity in our model and prioritizes fewer parameters. Ridge regression, on the other hand, is effective for dealing with multicollinearity (identified by VIF), when features are highly correlated, by shrinking the magnitude of the coefficients. According to the VIF calculated previously (greater than 1), our data has high multicollinearity, which can be potentially addressed using the ridge regression. 

```{r ridge regression and lasso}
set.seed(123)

#We're first implementing lasso/ridge with balanced data with no feature selection 

X <- model.matrix(RainTomorrow~., data=rain_balanced)
X <- X[,-1]

X_train <- model.matrix(RainTomorrow~., data=rain_balanced_train)
X_test <- model.matrix(RainTomorrow~., data=rain_balanced_test)
X_train <- X_train[,-1]
X_test <- X_test[,-1]

# Target vector
y <- rain_balanced$RainTomorrow
y_train <- rain_balanced_train$RainTomorrow
y_test <- rain_balanced_test$RainTomorrow

# alpha=0 for ridge, alpha=1 (default) for lasso

# Ridge Regression (L2)

ridge_cv <- cv.glmnet(X_train, y_train, alpha=0, family = "binomial", type.measure = "class")
plot(ridge_cv)

# To select best lambda 
lambda_opt_ridge <- ridge_cv$lambda.min
lambda_opt_ridge

pred_ridge<- predict(ridge_cv, X_test, type = "class", s = lambda_opt_ridge)
table(y_test, pred_ridge)

#Lasso Regression (L1)
lasso_cv <- cv.glmnet(X_train, y_train, alpha=1, family = "binomial", type.measure = "class")
plot(lasso_cv)

lambda_opt_lasso <- lasso_cv$lambda.min
lambda_opt_lasso

pred_lasso<- predict(lasso_cv, X_test, type = "class", s = lambda_opt_lasso)
table(y_test, pred_lasso)


#Then, we implemented Ridge and LASSO after feature selection

# Ridge Regression (L2)

ridge_cv_subset <- cv.glmnet(X_train_subset, y_train_subset, alpha=0, family = "binomial", type.measure = "class")
plot(ridge_cv_subset)

# To select best lambda 
lambda_opt_ridge_subset <- ridge_cv_subset$lambda.min
lambda_opt_ridge_subset

pred_ridge_subset<- predict(ridge_cv_subset, X_test_subset, type = "class", s = lambda_opt_ridge_subset)
table(y_test_subset, pred_ridge_subset)

#Lasso Regression (L1)
lasso_cv_subset <- cv.glmnet(X_train_subset, y_train_subset, alpha=1, family = "binomial", type.measure = "class")
plot(lasso_cv_subset)

lambda_opt_lasso_subset <- lasso_cv_subset$lambda.min
lambda_opt_lasso_subset

pred_lasso_subset<- predict(lasso_cv_subset, X_test_subset, type = "class", s = lambda_opt_lasso_subset)
table(y_test_subset, pred_lasso_subset)

```

## Linear and Quadratic Discriminant Analysis
Linear and Quadratic Discriminant Analysis, also known as LDA and QDA, fall into another class of statistical modeling techniques that can be used in supervised learning. 
LDA projects high-dimensional data into a lower-dimensional space while still preserving the original classifications of the training set. The reducing technique separates the data linearly to deduce a linear combination of the features that best generalize the data. LDA is best suited for data with clear spatial separation of classes and assumes the equality of covariance matrices between the classes. 
Meanwhile, QDA is an extension of LDA that can capture non-linear relationships across model features. Unlike LDA, QDA is still suitable when the classes have unique covariance matrices.In our project, we performed both LDA and QDA on our balanced and feature selected data. However, since LDA and QDA are sensitive to outliers, we performed outlier removal prior to running the models.

###Remove outliers from data 
```{r Remove outliers}
#looking for outliers in our data after balancing and feature selection 

g1<- ggplot(data = rain_subset_train, aes(y = MinTemp,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Min Temperature")
chisq.out.test(rain_subset_train$MinTemp)  #p-value = 0.00151, remove 376
which(rain_subset_train$MinTemp == 0) 


g2<- ggplot(data = rain_subset_train, aes(y = Sunshine,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Sunshine")
chisq.out.test(rain_subset_train$Sunshine)  #p-value = 0.04431, index 6965 
which(rain_subset_train$Sunshine == 1) 


g3<- ggplot(data = rain_subset_train, aes(y = WindGustSpeed,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("WindGustSpeed")
#print(g3)
chisq.out.test(rain_subset_train$WindGustSpeed) #p-value = 3.071e-07, no values 
which(rain_subset_train$WindGustSpeed == 0.939130434782609) 


g4<- ggplot(data = rain_subset_train, aes(y = WindSpeed9am,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("WindSpeed9am")
#print(g4)
chisq.out.test(rain_subset_train$WindSpeed9am)  #p-value = 1.43e-08 , no values
which(rain_subset_train$WindSpeed9am == 0.969230769230769) 


g5<- ggplot(data = rain_subset_train, aes(y = WindSpeed3pm,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("WindSpeed3pm")
#print(g5)
chisq.out.test(rain_subset_train$WindSpeed3pm)  #p-value = 4.733e-07 , no values
which(rain_subset_train$WindSpeed3pm == 0.851351351351351) 


g6<- ggplot(data = rain_subset_train, aes(y = Humidity3pm,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Humidity3pm")
#print(g6)
chisq.out.test(rain_subset_train$Humidity3pm) #p-value = 0.009785, indices:3782 10189 10959 15362 16105 18240
which(rain_subset_train$Humidity3pm ==0.01)


g7<- ggplot(data = rain_subset_train, aes(y = Pressure9am,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Pressure9am")
#print(g7)
chisq.out.test(rain_subset_train$Pressure9am) # p-value = 2.435e-06, index= 1935
which(rain_subset_train$Pressure9am ==0.0283806343906518)


g8<- ggplot(data = rain_subset_train, aes(y = Pressure3pm,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Pressure3pm")
#print(g8)
chisq.out.test(rain_subset_train$Pressure3pm)  # p-value = 2.756e-07, index=15369
which(rain_subset_train$Pressure3pm ==0)


g9<- ggplot(data = rain_subset_train, aes(y = Cloud9am,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Cloud9am")
#print(g9)
chisq.out.test(rain_subset_train$Cloud9am)
# which(rain_subset_train$Cloud9am ==0) # we got a p-value of 0.07 so we cannot refute the null hypothesis 


g10<- ggplot(data = rain_subset_train, aes(y = Cloud3pm,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Cloud3pm")
#print(g10)
chisq.out.test(rain_subset_train$Cloud3pm) #p-value = 0.04988 
#which(rain_subset_train$Cloud3pm ==0)


g11<- ggplot(data = rain_subset_train, aes(y = Temp3pm,fill = 2)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2)+
theme(legend.position="none") +
ylab("Temp3pm")
#print(g11)
chisq.out.test(rain_subset_train$Temp3pm)#p-value = 0.0003997, indices= 4596, 10679
which(rain_subset_train$Temp3pm ==1)


grid.arrange(g1, g2, g3,g4,g5,g6,g7,g8,g9,g10,g11,  nrow = 3)

#remove outliers for p_values less that 0.05
rain_subset_train_NoOutliers <- rain_subset_train[-c(4596,10679,15369,1935,3782,10189,10959,15362,16105,18240,376,6965)]

```
###LDA
```{r LDA}
# Model definition starting from the previous glm_bal model:

lda<- lda(data = rain_subset_train_NoOutliers,RainTomorrow ~.,family = "binomial")
lda

pred_lda<- predict(lda, rain_subset_test, type = "response")

post_lda<- pred_lda$posterior

pred_lda_04<- as.factor(ifelse(post_lda[,2] > threshold4, 1, 0))
pred_lda_05<- as.factor(ifelse(post_lda[,2] > threshold5, 1, 0))
pred_lda_06<- as.factor(ifelse(post_lda[,2] > threshold6, 1, 0))


# Confusion matrix with threshold = 0.4
error_lda4 <- mean(pred_lda_04!=rain_subset_test$RainTomorrow)
accuracy_lda4 <- mean(pred_lda_04==rain_subset_test$RainTomorrow)
lda_CM04 <- confusionMatrix(data = factor(pred_lda_04), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
error_lda5 <- mean(pred_lda_05!=rain_subset_test$RainTomorrow)
accuracy_lda5 <- mean(pred_lda_05==rain_subset_test$RainTomorrow)
lda_CM05 <- confusionMatrix(data = factor(pred_lda_05), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
error_lda6 <- mean(pred_lda_06!=rain_subset_test$RainTomorrow)
accuracy_lda6 <- mean(pred_lda_06==rain_subset_test$RainTomorrow)
lda_CM06 <- confusionMatrix(data = factor(pred_lda_06), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')



A <- create_confusion_matrix(lda_CM04, 0.4, error_lda4, accuracy_lda4)
B <- create_confusion_matrix(lda_CM05, 0.5, error_lda5, accuracy_lda5)
C <- create_confusion_matrix(lda_CM06, 0.6, error_lda6, accuracy_lda6)

# Threshold of 0.6 is the best among thresholds in terms of accuracy, sensitivity, and specificity
CM_all_lda = list(A,B,C)
plot_width <- c(4, 4, 4)
grid.arrange(grobs = CM_all_lda, nrow = 3, width = plot_width)
```
```{r LDA histogram}
# We use now the information given by:
# - x: linear combination of the variables that better describe the examples
# - class: assigned class

ldahist(pred_lda$x[,1], g = pred_lda$class, col = 2)
```

###QDA
```{r QDA}
qda<- qda(data = rain_subset_train_NoOutliers,RainTomorrow ~.,family = "binomial")
qda

pred_qda<- predict(qda, rain_subset_test, type = "response")

post_qda<- pred_qda$posterior
 
pred_qda_04<- as.factor(ifelse(post_qda[,2] > threshold4, 1, 0))
pred_qda_05<- as.factor(ifelse(post_qda[,2] > threshold5, 1, 0))
pred_qda_06<- as.factor(ifelse(post_qda[,2] > threshold6, 1, 0))


# Confusion matrix with threshold = 0.4
error_qda4 <- mean(pred_qda_04!=rain_subset_test$RainTomorrow)
accuracy_qda4 <- mean(pred_qda_04==rain_subset_test$RainTomorrow)
qda_CM04 <- confusionMatrix(data = factor(pred_qda_04), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.5
error_qda5 <- mean(pred_qda_05!=rain_subset_test$RainTomorrow)
accuracy_qda5 <- mean(pred_qda_05==rain_subset_test$RainTomorrow)
qda_CM05 <- confusionMatrix(data = factor(pred_qda_05), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

# Confusion matrix with threshold = 0.6
error_qda6 <- mean(pred_qda_06!=rain_subset_test$RainTomorrow)
accuracy_qda6 <- mean(pred_qda_06==rain_subset_test$RainTomorrow)
qda_CM06 <- confusionMatrix(data = factor(pred_qda_06), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')

A <- create_confusion_matrix(qda_CM04, 0.4, error_qda4, accuracy_qda4)
B <- create_confusion_matrix(qda_CM05, 0.5, error_qda5, accuracy_qda5)
C <- create_confusion_matrix(qda_CM06, 0.6, error_qda6, accuracy_qda6)

# Threshold of 0.05 is the best among thresholds in terms of accuracy, sensitivity, and specificity
CM_all_qda = list(A,B,C)
plot_width <- c(4, 4, 4) 
grid.arrange(grobs = CM_all_qda, nrow = 3, width = plot_width)

```

## K-Nearest Neighbors 

K-Nearest Neighbors (KNN) is a non-parametric, supervised learning technique, which assumes that features that are close in the feature space belong to the same class. The model identifies the labeled K-closest points to an observed point, and assigns the point to the majority class of the said-K-nearest neighbors. KNN is very sensitive to feature scaling and choice of K. If K is too small, KNN might lead to overfitting as each point is classified according to a small set of training examples. Choosing a K that is too large, however, may lead to less overfitting, but with the tradeoff of higher bias and too simple of a model.

```{r KNN}
set.seed(2531)

# We look now for the best value of the parameter 
kmax <- 100
knn_test_error <- numeric(kmax)

# For each possible value of k we consider the obtained accuracy of the model
for(k in 1:kmax) 
  {
  knn_pred <- as.factor(knn(X_train_subset,X_test_subset,cl = y_train_subset, k = k))
  
  cm <- confusionMatrix(data = knn_pred, reference = y_test_subset)

  knn_test_error[k] <- 1 - cm$overall[1]
  }

# We took the minimum value of the error
k_min <- which.min(knn_test_error)
k_min

# We compute now the prediction with the value of k that gives us the minimum error
knn<- knn(X_train_subset, X_test_subset,cl = y_train_subset, k = k_min)

knn_pred_min <- knn

# Confusion matrix for KNN on the test set
tab<- table(y_test_subset, knn)
tab
```

```{r plot error}
#Plot test error against different levels of K
ggplot(data.frame(knn_test_error), 
      aes(x = 1:kmax, y = knn_test_error)) +
      geom_line(colour="blue") +
      geom_point(colour="red") +
      xlab("K (#neighbors)") + 
      ylab("Test error") +
      ggtitle(paste0("Best value of K = ", k_min,
                     " (minimal error = ",
                    format((knn_test_error[k_min])*100, digits = 4), 
                    "%)"))
```
#TODO: Analysis, Clean Visualizations and Code

## Analysis
```{r Analysis }






```