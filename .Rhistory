table(rain_n_test0$RainTomorrow, glm_predict_4_0)
mean(glm_predict_4_0!=rain_n_test0$RainTomorrow)
cm4_0 <- confusionMatrix(data = factor(glm_predict_4_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')
# Confusion matrix with threshold = 0.5
table(rain_n_test0$RainTomorrow, glm_predict_5_0)
mean(glm_predict_5_0!=rain_n_test0$RainTomorrow)
cm5_0 <- confusionMatrix(data = factor(glm_predict_5_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')
# Confusion matrix with threshold = 0.6
table(rain_n_test0$RainTomorrow, glm_predict_6_0)
mean(glm_predict_6_0!=rain_n_test0$RainTomorrow)
cm6_0 <- confusionMatrix(data = factor(glm_predict_6_0), reference = factor(rain_n_test0$RainTomorrow), mode ='everything')
a0 <- create_confusion_matrix(cm4_0)
b0 <- create_confusion_matrix(cm5_0)
c0 <- create_confusion_matrix(cm6_0)
# Threshold of 0.05 is the best among thresholds in terms of accuracy, sensitivity, and specificity
cm_all0 = list(a0, b0, c0)
plot_width <- c(4, 4, 4)
grid.arrange(grobs = cm_all0, nrow = 3, width = plot_width)
#Downsamples majority class(0)
#Added yname to specify the target variable in downSample function, ow it assumes first col is target
rain_balanced <- downSample(x = rain_n[, -which(names(rain_n) == "RainTomorrow")],
y = factor(rain_n$RainTomorrow),
yname = "RainTomorrow")
table(rain_balanced$RainTomorrow)
head(rain_balanced)
#Check if there are any NAs
sum(is.na(rain_balanced$RainTomorrow))
#Test/Train Split
set.seed(123)
train_balanced <- sample(1:nrow(rain_balanced), nrow(rain_balanced) * 0.75)
# Calculate the test indices
test_balanced <- setdiff(1:nrow(rain_balanced), train_balanced)
# Split the target variable into train and test sets
rain_balanced_train <- rain_balanced[train_balanced,]
rain_balanced_test <- rain_balanced[test_balanced ,]
glm_model_balanced <- glm(data = rain_balanced_train,
rain_balanced_train$RainTomorrow ~ .,
family = binomial)
#R squared and Variance Inflation Factor (VIF)
#If the VIF value for a predictor variable is greater than 1, it indicates the presence of multicollinearity, suggesting that the predictor variable is highly correlated with other predictor variables in the model.
model_summary_balanced <- summary(glm_model_balanced)
summary(glm_model_balanced)
r2_balanced <- 1 - (model_summary_balanced$deviance/model_summary_balanced$null.deviance) # 0.3849359
vif_balanced <- 1/(1-r2_balanced) # 1.625847
#Predict test with model
glm_predict_balanced <- predict(glm_model_balanced, rain_balanced_test, type = "response")
#Convert predictions into 0,1 based on different thresholds
glm_predict_4_balanced<- ifelse(glm_predict_balanced > threshold4, 1, 0)
glm_predict_5_balanced<- ifelse(glm_predict_balanced > threshold5, 1, 0)
glm_predict_6_balanced<- ifelse(glm_predict_balanced > threshold6, 1, 0)
# Confusion matrix with threshold = 0.4
table(rain_balanced_test$RainTomorrow, glm_predict_4_balanced)
mean(glm_predict_4_balanced!=rain_balanced_test$RainTomorrow)
cm4_balanced <- confusionMatrix(data = factor(glm_predict_4_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')
# Confusion matrix with threshold = 0.5
table(rain_balanced_test$RainTomorrow, glm_predict_5_balanced)
mean(glm_predict_5_balanced!=rain_balanced_test$RainTomorrow)
cm5_balanced <- confusionMatrix(data = factor(glm_predict_5_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')
# Confusion matrix with threshold = 0.6
table(rain_balanced_test$RainTomorrow, glm_predict_6_balanced)
mean(glm_predict_6_balanced!=rain_balanced_test$RainTomorrow)
cm6_balanced <- confusionMatrix(data = factor(glm_predict_6_balanced), reference = factor(rain_balanced_test$RainTomorrow), mode ='everything')
a_balanced <- create_confusion_matrix(cm4_balanced)
b_balanced <- create_confusion_matrix(cm5_balanced)
c_balanced <- create_confusion_matrix(cm6_balanced)
# Threshold of 0.05 is the best among thresholds in terms of accuracy, sensitivity, and specificity
cm_all_balanced = list(a_balanced, b_balanced, c_balanced)
plot_width <- c(4, 4, 4)
grid.arrange(grobs = cm_all_balanced, nrow = 3, width = plot_width)
# Perform logistic regression with backward stepwise selection
logit_model <- glm(rain_balanced$RainTomorrow ~ ., data = rain_balanced, family = binomial)
# Perform forward stepwise selection using AIC with log(n)
logit_model <- stepAIC(logit_model, direction = "backward", k = log(nrow(rain_balanced)), trace = FALSE)
# Print the summary of the logistic regression model
summary(logit_model)
# Subset the dataframe with the chosen features based on stepwise selection
rain_subset <- rain_balanced[, c("MinTemp", "Sunshine", "WindGustSpeed", "WindSpeed9am", "WindSpeed3pm", "Humidity3pm", "Pressure9am", "Pressure3pm", "Cloud9am", "Cloud3pm", "Temp3pm", "RainToday", "RainTomorrow")]
set.seed(123)
train <- sample(1:nrow(rain_subset), nrow(rain_subset) * 0.75)
# Calculate the test indices
test <- setdiff(1:nrow(rain_subset), train)
# Split the target variable into train and test sets
rain_subset_train <- rain_subset[train,]
rain_subset_test <- rain_subset[test,]
head(rain_subset_train)
head(rain_subset_test)
# Model Definition
glm_model <- glm(data = rain_subset_train,
rain_subset_train$RainTomorrow ~ .,
family = binomial)
#R squared and Variance Inflation Factor (VIF)
#If the VIF value for a predictor variable is greater than 1, it indicates the presence of multicollinearity, suggesting that the predictor variable is highly correlated with other predictor variables in the model.
model_summary <- summary(glm_model)
summary(glm_model)
r2 <- 1 - (model_summary$deviance/model_summary$null.deviance) # 0.3845522
vif <- 1/(1-r2) # 1.624833
#Predict test with model
glm_predict <- predict(glm_model, rain_subset_test, type = "response")
#Convert predictions into 0,1 based on different thresholds
glm_predict_4<- ifelse(glm_predict > threshold4, 1, 0)
glm_predict_5<- ifelse(glm_predict > threshold5, 1, 0)
glm_predict_6<- ifelse(glm_predict > threshold6, 1, 0)
# Confusion matrix with threshold = 0.4
table(rain_subset_test$RainTomorrow, glm_predict_4)
mean(glm_predict_4!=rain_subset_test$RainTomorrow)
cm4 <- confusionMatrix(data = factor(glm_predict_4), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')
# Confusion matrix with threshold = 0.5
table(rain_subset_test$RainTomorrow, glm_predict_5)
mean(glm_predict_5!=rain_subset_test$RainTomorrow)
cm5 <- confusionMatrix(data = factor(glm_predict_5), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')
# Confusion matrix with threshold = 0.6
table(rain_subset_test$RainTomorrow, glm_predict_6)
mean(glm_predict_6!=rain_subset_test$RainTomorrow)
cm6 <- confusionMatrix(data = factor(glm_predict_6), reference = factor(rain_subset_test$RainTomorrow), mode ='everything')
a <- create_confusion_matrix(cm4)
b <- create_confusion_matrix(cm5)
c <- create_confusion_matrix(cm6)
cm_all = list(a, b, c)
plot_width <- c(4, 4, 4)
grid.arrange(grobs = cm_all, nrow = 3, width = plot_width)
## Summary Statistics: F1-Score
models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c("_balanced", "_0", "")
thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)
f1_scores <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), stringsAsFactors = FALSE)
for (model in models) {
for (i in 1:length(thresholds)) {
threshold <- thresholds[i]
threshold_value <- threshold_values[i]
# Calculate the F1 score for each combination of model and threshold
cm_name <- paste0("cm", thresholds[i], model_suffix[i])
cm <- get(cm_name)
f1_score <- cm$byClass["F1"]
# Add the F1 score to the data frame
f1_scores <- rbind(f1_scores, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, stringsAsFactors = FALSE))
}
}
f1_scores
## Ridge Regression
# Fitted Model
glm_ridge <- glm(data = rain_balanced,
rain_balanced$RainTomorrow ~ .,
family = binomial)
# compute ell_1 and ell_2 norm for ls estimates
beta_hat_ls <- coefficients(glm_ridge)[-1]
ell_1_ls <- sum(abs(beta_hat_ls))
ell_1_ls
ell_2_ls <- sqrt(sum(beta_hat_ls^2))
ell_2_ls
# grid of lambda values
grid <- 10^seq(10, -2, length=100)
plot(grid, type="l")
plot(log10(grid), type="l")
summary(log10(grid))
plot(log(grid), type="l")
summary(log(grid))
# Apply glmnet() function
# design matrix
#
X <- model.matrix(RainTomorrow~., data=rain_balanced)
# remove the first column relative to the intercept
#
X <- X[,-1]
# vector of responses
y <- rain_balanced$RainTomorrow
# alpha=0 for ridge, alpha=1 (default) for lasso
#
# ridge is applied to standardized variables,
# use standardize=FALSE to turn off this setting
#
# if lambda is not given glmnet() chooses its own sequence
ridge_mod <- glmnet(X, y, alpha=0, lambda=grid, thresh = 1e-12, family = "binomial")
# plot method for glmnet
#  xvar = c("norm", "lambda", "dev")
# add labels to identify the variables
# Plots include lines that represent each coefficients. The closer value of variable
# is to 0, the more the shrinkage has been applied.
# Log lamda represents the L2 norm
plot(ridge_mod, xvar="lambda", label=TRUE)
plot(ridge_mod, xvar="norm", label=TRUE)
plot(ridge_mod, xvar="dev", label=TRUE)
# coef() gives a matrix of coefficients
dim(coef(ridge_mod))
# coefficients for lambda[50]
ridge_mod$lambda[50]
coef(ridge_mod)[,50]
# ell_2 norm and amount of shrinkage
sqrt(sum(coef(ridge_mod)[-1,50]^2))
sqrt(sum(coef(ridge_mod)[-1,50]^2))/ell_2_ls
plot(ridge_mod, xvar="lambda", label=TRUE)
abline(v=log(ridge_mod$lambda[50]), lty=3, lwd=2)
# coefficients for lambda[60]
ridge_mod$lambda[70]
coef(ridge_mod)[,70]
# # ell_2 norm and amount of shrinkage
sqrt(sum(coef(ridge_mod)[-1,70]^2))
sqrt(sum(coef(ridge_mod)[-1,70]^2))/ell_2_ls
abline(v=log(ridge_mod$lambda[70]), lty=3, lwd=2, col="red")
# coefficients for any lambda with interpolation
# s=lambda (interpolates over the grid of lambda used in fitting)
ridge_mod$lambda[50]
ridge_mod$lambda[49]
coef(ridge_mod, s=15000)
# use the predict function for a number of purposes
# s=lambda (interpolates over the grid of lambda used in fitting)
# type="coefficients" returns the beta-values
# type="response" returns the y.hat values
predict(ridge_mod, s=15000, type="coefficients")
y_hat <- predict(ridge_mod, s=15000, newx=X, type="response")
## Test Ridge Regression on Train/Test Split
set.seed(123)
X_train <- model.matrix(RainTomorrow~., data=rain_balanced_train)
X_test <- model.matrix(RainTomorrow~., data=rain_balanced_test)
X_train <- X_train[,-1]
X_test <- X_test[,-1]
y_train <- rain_balanced_train$RainTomorrow
y_test <- rain_balanced_test$RainTomorrow
## Ridge Regression
# Fitted Model
glm_ridge <- glm(data = rain_balanced,
rain_balanced$RainTomorrow ~ .,
family = binomial)
# compute ell_1 and ell_2 norm for ls estimates
beta_hat_ls <- coefficients(glm_ridge)[-1]
ell_1_ls <- sum(abs(beta_hat_ls))
ell_1_ls
ell_2_ls <- sqrt(sum(beta_hat_ls^2))
ell_2_ls
# grid of lambda values
grid <- 10^seq(10, -2, length=100)
plot(grid, type="l")
plot(log10(grid), type="l")
summary(log10(grid))
plot(log(grid), type="l")
summary(log(grid))
# Apply glmnet() function
# design matrix
#
X <- model.matrix(RainTomorrow~., data=rain_balanced)
# remove the first column relative to the intercept
#
X <- X[,-1]
# vector of responses
y <- rain_balanced$RainTomorrow
# alpha=0 for ridge, alpha=1 (default) for lasso
#
# ridge is applied to standardized variables,
# use standardize=FALSE to turn off this setting
#
# if lambda is not given glmnet() chooses its own sequence
ridge_mod <- glmnet(X, y, alpha=0, lambda=grid, thresh = 1e-12, family = "binomial")
# plot method for glmnet
#  xvar = c("norm", "lambda", "dev")
# add labels to identify the variables
# Plots include lines that represent each coefficients. The closer value of variable
# is to 0, the more the shrinkage has been applied.
# Log lamda represents the L2 norm
plot(ridge_mod, xvar="lambda", label=TRUE)
plot(ridge_mod, xvar="norm", label=TRUE)
plot(ridge_mod, xvar="dev", label=TRUE)
# coef() gives a matrix of coefficients
dim(coef(ridge_mod))
# coefficients for lambda[50]
ridge_mod$lambda[50]
coef(ridge_mod)[,50]
# ell_2 norm and amount of shrinkage
sqrt(sum(coef(ridge_mod)[-1,50]^2))
sqrt(sum(coef(ridge_mod)[-1,50]^2))/ell_2_ls
plot(ridge_mod, xvar="lambda", label=TRUE)
abline(v=log(ridge_mod$lambda[50]), lty=3, lwd=2)
# coefficients for lambda[60]
ridge_mod$lambda[70]
coef(ridge_mod)[,70]
# # ell_2 norm and amount of shrinkage
sqrt(sum(coef(ridge_mod)[-1,70]^2))
sqrt(sum(coef(ridge_mod)[-1,70]^2))/ell_2_ls
abline(v=log(ridge_mod$lambda[70]), lty=3, lwd=2, col="red")
# coefficients for any lambda with interpolation
# s=lambda (interpolates over the grid of lambda used in fitting)
ridge_mod$lambda[50]
ridge_mod$lambda[49]
coef(ridge_mod, s=15000)
# to select best lambda
ridge_cv <- cv.glmnet(X, y, alpha=0, family = "binomial", type.measure = "class")
plot(ridge_cv)
# use the predict function for a number of purposes
# s=lambda (interpolates over the grid of lambda used in fitting)
# type="coefficients" returns the beta-values
# type="response" returns the y.hat values
predict(ridge_mod, s=15000, type="coefficients")
y_hat <- predict(ridge_mod, s=15000, newx=X, type="response")
## Test Ridge Regression on Train/Test Split
set.seed(123)
X_train <- model.matrix(RainTomorrow~., data=rain_balanced_train)
X_test <- model.matrix(RainTomorrow~., data=rain_balanced_test)
X_train <- X_train[,-1]
X_test <- X_test[,-1]
y_train <- rain_balanced_train$RainTomorrow
y_test <- rain_balanced_test$RainTomorrow
lambda_opt_ridge <- ridge_cv$lambda.min
lambda_opt_ridge
ridge_cv <- cv.glmnet(X_train, y_train, alpha=0, family = "binomial", type.measure = "class")
plot(ridge_cv)
lambda_opt_ridge <- ridge_cv$lambda.min
lambda_opt_ridge
pred_ridge<- predict(ridge_cv, X_test, type = "class", s = lambda_opt_ridge)
table(y_test, pred_ridge)
lasso_cv <- cv.glmnet(X_train, y_train, alpha=1, family = "binomial", type.measure = "class")
plot(lasso_cv)
lambda_opt_lasso <- lasso_cv$lambda.min
lambda_opt_lasso
pred_lasso<- predict(lasso_cv, X_test, type = "class", s = lambda_opt_lasso)
table(y_test, pred_lasso)
set.seed(123)
X <- model.matrix(RainTomorrow~., data=rain_balanced)
X <- X[,-1]
X_train <- model.matrix(RainTomorrow~., data=rain_balanced_train)
X_test <- model.matrix(RainTomorrow~., data=rain_balanced_test)
X_train <- X_train[,-1]
X_test <- X_test[,-1]
# Target vector
y <- rain_balanced$RainTomorrow
y_train <- rain_balanced_train$RainTomorrow
y_test <- rain_balanced_test$RainTomorrow
# alpha=0 for ridge, alpha=1 (default) for lasso
# Ridge Regression (L2)
ridge_cv <- cv.glmnet(X_train, y_train, alpha=0, family = "binomial", type.measure = "class")
plot(ridge_cv)
# to select best lambda
lambda_opt_ridge <- ridge_cv$lambda.min
lambda_opt_ridge
pred_ridge<- predict(ridge_cv, X_test, type = "class", s = lambda_opt_ridge)
table(y_test, pred_ridge)
#Lasso Regression (L1)
lasso_cv <- cv.glmnet(X_train, y_train, alpha=1, family = "binomial", type.measure = "class")
plot(lasso_cv)
lambda_opt_lasso <- lasso_cv$lambda.min
lambda_opt_lasso
pred_lasso<- predict(lasso_cv, X_test, type = "class", s = lambda_opt_lasso)
table(y_test, pred_lasso)
f1_scores <- subset(f1_scores, select = -1)
print(f1_score, include.rownames = FALSE)
print(f1_scores, include.rownames = FALSE)
rownames(f1_scores) <- NULL
f1_scores
rownames(f1_scores) <- NULL
print(f1_scores)
## Summary Statistics: F1-Score
models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c("_balanced", "_0", "")
thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)
f1_scores <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), stringsAsFactors = FALSE)
for (model in models) {
for (i in 1:length(thresholds)) {
threshold <- thresholds[i]
threshold_value <- threshold_values[i]
# Calculate the F1 score for each combination of model and threshold
cm_name <- paste0("cm", thresholds[i], model_suffix[i])
cm <- get(cm_name)
f1_score <- cm$byClass["F1"]
# Add the F1 score to the data frame
f1_scores <- rbind(f1_scores, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, stringsAsFactors = FALSE))
}
}
f1_scores
print(f1_scores)
print(f1_scores,rnames=NULL)
print(f1_scores,row.names=NULL)
## Summary Statistics: F1-Score
models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c("_balanced", "_0", "")
thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)
f1_scores <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), stringsAsFactors = FALSE)
for (model in models) {
for (i in 1:length(thresholds)) {
threshold <- thresholds[i]
threshold_value <- threshold_values[i]
# Calculate the F1 score for each combination of model and threshold
cm_name <- paste0("cm", thresholds[i], model_suffix[i])
cm <- get(cm_name)
f1_score <- cm$byClass["F1"]
# Add the F1 score to the data frame
f1_scores <- rbind(f1_scores, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, stringsAsFactors = FALSE))
}
}
f1_scores
#new_df <- data.frame(old_df, row.names = NULL)
f1_scores
new_df <- data.frame(f1_scores, row.names = NULL)
new_df
## Summary Statistics: F1-Score
models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c("_balanced", "_0", "")
thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)
f1_scores <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), stringsAsFactors = FALSE)
for (model in models) {
for (i in 1:length(thresholds)) {
threshold <- thresholds[i]
threshold_value <- threshold_values[i]
# Calculate the F1 score for each combination of model and threshold
cm_name <- paste0("cm", thresholds[i], model_suffix[i])
cm <- get(cm_name)
f1_score <- cm$byClass["F1"]
# Add the F1 score to the data frame
f1_scores <- rbind(f1_scores, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, stringsAsFactors = FALSE))
}
}
f1_scores <- data.frame(f1_scores, row.names = NULL)
## Summary Statistics: F1-Score
models <- c("Simple GLM", "GLM with Balancing", "GLM with Balancing and Feature Selection")
model_suffix <- c("_balanced", "_0", "")
thresholds <- c(4, 5, 6)
threshold_values <- c(0.4, 0.5, 0.6)
f1_scores <- data.frame(Model = character(), Threshold = numeric(), F1_Score = numeric(), stringsAsFactors = FALSE)
for (model in models) {
for (i in 1:length(thresholds)) {
threshold <- thresholds[i]
threshold_value <- threshold_values[i]
# Calculate the F1 score for each combination of model and threshold
cm_name <- paste0("cm", thresholds[i], model_suffix[i])
cm <- get(cm_name)
f1_score <- cm$byClass["F1"]
# Add the F1 score to the data frame
f1_scores <- rbind(f1_scores, data.frame(Model = model, Threshold = threshold_value, F1_Score = f1_score, stringsAsFactors = FALSE,row.names = NULL))
}
}
f1_scores
# original classification :
ggplot(rain_n_test0 , aes(x = sunshine,
y = RainTomorrow,
color = Haz_test)) +
geom_point()+
labs(x = "Absolute Magnitude",
y = "Minimum Orbit Intersection",
color = "Hazardous") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Sunshine,
y = RainTomorrow,
color = Haz_test)) +
geom_point()+
labs(x = "Sunshine",
y = "Rain Tomorrow",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Sunshine,
y = RainTomorrow,
color =RainTomorrow )) +
geom_point()+
labs(x = "Sunshine",
y = "Rain Tomorrow",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Sunshine,
y = MazTemp,
color =RainTomorrow )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Sunshine,
y = MaxTemp,
color =RainTomorrow )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Sunshine,
y = MaxTemp,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Sunshine,
y = MinTemp,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x =Cloud9am ,
y = Rainfall,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
# original classification :
ggplot(rain_n_test0 , aes(x = Rainfall,
y = Cloud9am,
color= as.factor(RainTomorrow) )) +
geom_point()+
labs(x = "Sunshine",
y = "Max Temperature",
color = "Rain Tomorrow") +
theme(legend.position = c(0.8, 0.8))
